---
title: 'High Energy Physics and Energy-Based Learning'
date: 2022-03-21
permalink: /posts/2022/03/hep-ebl/
layout: post
usemathjax: true
tags:
  - projects and papers
  - reading guides
---

Last week, I released a [set of results](https://twitter.com/DaltonSakthi/status/1504093956270866433?s=20&t=6fdfM_AROWfDGQC_xYrGPw) proving some identities from gauge theory in the context of maximum entropy (preprint arXiv:2203.08119). In a more detailed sketch than the linked tweet, but staying less formal than the linked paper, I'll comment a little on the 'hidden stuff' in that paper—this should help readers read between the lines.[^1] 

I have [previously](https://arxiv.org/abs/2102.04896) [taken](https://darsakthi.github.io/talks/tnb-0) [the](https://darsakthi.github.io/talks/sbu-0) [position](https://darsakthi.github.io/talks/ttu-0) that inference is secretly a really high-tech way of doing analysis (i.e., the mathematical study of functions and their outputs). This is not strictly a new idea, but all the same, it has actually been foundational to my research—that statistical inference is a thing which might be of broad interest to pure mathematicians, and that describing inference correctly and completely is a mathematical problem with a mathematical solution, was part of the key to starting a viable research programme in mathematical complex systems theory. In any case, this sort of suggests that statistical inference is reducible to a big integral for a really big stochastic dynamical system describing one's data; by learning the relationships between data points, or between space-time inputs and data outputs, inference effectively solves a process generating that data, without knowing either the process _a priori_ or the solution to that process. That is, somehow, model selection and model solution coincide, such that we don't really _need_ to do analysis. 

In this paper, I take a slightly new position which corresponds to this observation—we don't actually solve anything when we do inference. In fact, we don't care about the underlying equation at all, and moreover, we don't need to. Inference works so well precisely _because_ in order to find the underlying process it stops caring about the underlying process. Instead, it studies the shape of the data, figuring out what the process ought to look like that way. This is similar in spirit to the way that humans employ intuitions about the dynamics of the world around us by using previous experiences about what's possible and what isn't, bypassing the need to actually calculate the action-minimising trajectories of bodies surrounding us. Here's how we might make that mathematically precise:

We start with a foundational model for statistical inference, which is called either constrained maximum entropy (by statistical physicists) or [energy-based learning](https://www.youtube.com/watch?v=piaPIKO1MFY) (by statistical theorists). There's a lot of work showing the ubiquity of constrained maximum entropy and EBL-type formalisms, so I won't go into that here, but they really are foundational. It's also slightly non-trivial that maximum entropy under appropriate constraints reproduces energy-based learning frameworks, but that's true as well (see [here](https://physicsofebm.github.io) for some demonstrations). 

The other thing we should begin with is not statistical, but high energy, physics. This is the physics of particles, fields, and strings, whose energy scales are much higher than everyday solids and liquids—what we call 'condensed matter.' If it isn't statistical, then what's the relationship? A key to the whole story is that one part of HEP, gauge field theory, really lends itself to the geometrisation of dynamical systems. A gauge potential is a choice of gauge at every point in space-time, which is just some quantity that affects the dynamics of a particle with a gauge symmetry (an intuitive overview of gauge theory is way beyond the scope of this post, but I might revisit it later. For now, think of every particle's state as having some context, like an arbitrary set of coordinates in which that state is expressed). My favourite discussion of this geometric relationship is in the opening to the famous paper by Baez and Schreiber, [Higher Gauge Theory](https://arxiv.org/abs/math/0511710)—or equivalently, the slightly more accessible [Invitation to Higher Gauge Theory](https://arxiv.org/abs/1003.4485) by Baez and Huerta. There, Baez and coauthors rely on an interpretation of something called _parallel transport_ as a geometric encoding of how a particle interacts with a gauge potential—how its state transforms—as it moves on a space-time trajectory. It wasn't a new idea at the time, but it really comes to life in those papers.  

So we have some useful ingredients—a mathematical/physical theory available to us via gauge theory, which tells us how dynamics in a space express geometric features of a space, and a connection to the mathematics and physics of inference via maximum entropy. Suppose we can bring these things together. Beyond the obvious utility of formulating an answer to our above question, there's something conceptually satisfying about bringing together the two most general theories available to modern physics (depending on who you ask, that's secondary to actually solving the problem). Indeed, we _can_ bring these together, in a really useful way. 

Imagine we have a dynamical system sampling from or generating the state space we are interested in. This system produces the data an inferential process tries to characterise. By placing constraints on different states (data points, say), we have both 1) a potential function for dynamics on the manifold we call the state space, and 2) an expression of preferences for different states in the space. This idea of preferences gives us a weighting on the probability of observing a given state which is inversely proportional to the constraint. Indeed, this is exactly what (one reading) of maximum entropy allows us to say. The first, more dynamical perspective is where we start from; it turns out we can recover the second view from the first, which is the whole point (and in some sense the main result) of the paper.

We just mentioned there is a principled way of speaking about dynamics under potentials with some geometric meaning. The core of the construction, at the intuitive level, is as follows: critically, we look at the flow of _probability_ through the state space. This is predicated on some process flowing on the state space—as it visits states, it maps them to a probabiity. So, the flow of probability through the state space ought to be determined by the potential function on the state space. Moreover, this two-way flow is what we would call a lift—a function associating a flow in some base space to a simultaneous flow in some other space. There is already a special sort of lift that obeys a constraining potential function, in that these lifts are the solutions to a Newtonian system of ODEs relating the dynamics of the curve exactly to the potential function—these are called horizontal lifts. The image of a horizontal lift is given by parallel transport. 

In more detail—when the potential can be read as a deformation of the space, geodesic curves on that deformed geometry can be thought of as being given by parallel transportation. Suppose we have an ensemble of states and want to lift those states into some other space encoding some extra information, like a space of scalars attaching a probability to each state. What that lift looks like now depends on the potential. When we do inference and try to learn probabilities over data, we are trying to find what that lift is—i.e., what is $$p(x)$$. Equating this with a simple flow in a potential has a nice interpretation as the classification of states by probability _not_ being the solution to a Fokker-Planck equation, but the parallel transport of points $$p(x)$$ as a quantity interacting with the potential on states. This view is implicitly taking $$(x, p(x))$$ as a _surface_ containing probabilistic points (that is, real scalar numbers, each associated to a state). Parallel transport means we describe the density as being made up of lines along some surface, whose shapes are determined by the constraints on the states underlying the surface. This in turn means a simple equation for a flow in a potential is sufficient to determine the shape of the density. No integrals necessary. So, incorporating into our lifting process the interactions of flows in the probability space over states with the potential gives a nice conceptual explanation of the effectiveness of maximum entropy, and should be possible using parallel transport. 

The equation for parallel transport satisfies a Newtonian relation for geodesic flows,

$$ \nabla \nabla p(x) = - \nabla (\nabla V(x) p(x)), \tag{1}$$

yielding the second-order spatial change in some point $$p(x)$$ under a potential function $$\grad V(x)$$. By the gradient theorem, when we integrate \ref{1} (ignoring some ultimately unimportant technical details) we get the system of ODEs for parallel transport in a potential $$\grad V(x)$$,

$$ \nabla p(x) = - \nabla V(x) p(x), $$

whose solution is obviously $$\exp\{ -V(x) \}$$. Attentive readers might recognise \ref{1} as the stationary Fokker-Planck equation; indeed, $$\exp\{ - V(x) \}$$ is the solution to that too. The equivalence is more principled still—the parallel transport equation rearranges to 

$$ - \frac{\nabla p(x)}{p(x)} - \nabla V(x), $$

which integrates to the Euler-Lagrange equation satisfying the maximisation of entropy subject to a constraint function, $$-\ln\{p(x)\} - V(x)$$. Maximising entropy is famously the stationary solution to the Fokker-Planck equation (see work by Otto, Villani, etc). So, the purely analytic perspective confirms the general idea. Establishing as much is the content of Theorem 1.

The point of the paper is rather geometric, though, in that we want to look at parallel transport as a geometric feature of a dynamical theory. In the sense that parallel transport flows encode the geometry of a state space in which something flows, this is certainly possible. 

The utility of these results are that they allow us to conceptualising inference as a horizontal lift. Flows in the state space are constrained by the shape of the data, so by exploring the state space there is a very precise sense in which one learns a description of the data. Here, that flow occurs at maximum entropy, by finding parallel transport lines; the description we learn is a probabilistic one about how likely or preferable a state is. 

[^1]: See Thurston, [On Proof and Progress in Mathematics](https://arxiv.org/abs/math/9404236), page seven. By 'hidden stuff,' I mean the unstated motivations for various things in the paper, or the 'point' of any given structure in the paper—whether a technical choice like the particular statement of a proof, or a stylistic choice like the order of exposition, etc. By reading between the lines, I (and Thurston) mean hacking an easy path through the paper, translating the information-dense encoding of the language of formal mathematics into something a little easier to read, with less extra adornment and more intution for what's essential. In a way, we're doing parallel transport over the paper, getting a feel for its shape, rather than computing its solution. This should make the paper a bit more palatable.
