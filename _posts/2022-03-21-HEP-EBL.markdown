---
title: 'High Energy Physics and Energy-Based Learning'
date: 2022-03-21
permalink: /posts/2022/03/hep-ebl/
layout: post
usemathjax: true
tags:
  - projects and papers
  - reading guides
---

Last week I released a [set of results](https://twitter.com/DaltonSakthi/status/1504093956270866433?s=20&t=6fdfM_AROWfDGQC_xYrGPw) proving some identities from gauge theory in the context of maximum entropy (preprint arXiv:2203.08119). In a more detailed sketch than the linked tweet, but staying less formal than the linked paper, I'll comment a little on the 'hidden stuff' in that paper—this should help you read between the lines (cf. W P Thurston, [On Proof and Progress in Mathematics](https://arxiv.org/abs/math/9404236), page seven), which might make it a bit more palatable.

I have [previously](https://arxiv.org/abs/2102.04896) [taken](https://darsakthi.github.io/talks/tnb-0) [the](https://darsakthi.github.io/talks/sbu-0) [position](https://darsakthi.github.io/talks/ttu-0) that inference is secretly a really fancy way of doing analysis (i.e., the mathematical study of functions and their outputs). This is not strictly a new idea, but all the same, it has actually been foundational to my research—that statistical inference is a thing which might be of broad interest to pure mathematicians, and that describing inference correctly and completely is a mathematical problem with a mathematical solution, was part of the key to starting a viable research programme in mathematical complex systems theory. In any case, this sort of suggests that statistical inference is reducible to a big integral for a really big stochastic dynamical system describing one's data; by learning the relationships between data points, or between space-time inputs and data outputs, inference effectively solves a process generating that data, without knowing either the process _a priori_ or the solution to that process. That is, somehow, model selection and model solution coincide, such that we don't really _need_ to do analysis. 

In this paper, I take a slightly new position which corresponds to this observation—we don't actually solve anything when we do inference. In fact, we don't care about the underlying equation at all. Inference works so well precisely _because_ in order to find the underlying process it stops caring about the underlying process. Instead, it studies the shape of the data, figuring out what the process ought to look like that way. This is similar in spirit to the same way that humans gain intuitions for the dynamics of the world, without actually calculating the action-minimising trajectories of bodies surrounding us. Here's how we make that mathematically precise:

We start with a foundational model for statistical inference, which is called either constrained maximum entropy (by statistical physicists) or [energy-based learning](https://www.youtube.com/watch?v=piaPIKO1MFY) (by statistical theorists). There's a lot of work showing the ubiquity of constrained maximum entropy and EBL-type formalisms, so I won't go into that here, but they really are foundational. It's also slightly non-trivial that maximum entropy under appropriate constraints reproduces energy-based learning frameworks, but that's true as well (see [here](https://physicsofebm.github.io) for a brief walkthrough). 

The other thing we should begin with is not statistical, but high energy, physics. This is the physics of particles, fields, and strings, whose energy scales are much higher than everyday condensed matter. If it isn't statistical, then what's the relationship? A key to the whole story is that one part of HEP, gauge field theory, really lends itself to the geometrisation of dynamical systems. A gauge potential is a choice of gauge at every point in space-time, which is just some quantity that affects the dynamics of a particle with a gauge symmetry (an intuitive overview of gauge theory is way beyond the scope of this post, but I might revisit it later. For now, think of every particle's state as having some context, like an arbitrary set of coordinates in which that state is expressed). My favourite discussion of this geometric relationship is in the opening to the famous paper by Baez and Schreiber, [Higher Gauge Theory](https://arxiv.org/abs/math/0511710)—or equivalently, the slightly more accessible [Invitation to Higher Gauge Theory](https://arxiv.org/abs/1003.4485) by Baez and Huerta. There, Baez and coauthors rely on an interpretation of something called _parallel transport_ as a geometric encoding of how a particle interacts with a gauge potential—how its state transforms—as it moves on a space-time trajectory. It wasn't a new idea at the time, but it really comes to life in those papers.  

So we have some useful ingredients—a mathematical/physical theory available to us via gauge theory, which expresses how dynamics in a space express geometric features of a space, and a connection to the mathematics and physics of inference via maximum entropy. Suppose we can bring these things together. Beyond the obvious utility of formulating an answer to our above question, there's something conceptually satisfying about bringing together the two most general theories available to modern physics (depending on who you ask, that's secondary to actually solving the problem). Indeed, we _can_ bring these together, in a really useful way. 

Imagine we have a dynamical system sampling from or generating the state space we are interested in. This system produces the data an inferential process tries to characterise. By placing constraints on different states (data points, say), we have both 1) a potential function for dynamics on the manifold we call the state space, and 2) an expression of preferences for different states in the space. This idea of preferences gives us a weighting on the probability of observing a given state which is inversely proportional to the constraint. Indeed, this is exactly what (one reading) of maximum entropy allows us to say. The first, more dynamical perspective is where we start from; it turns out we can recover the second view from the first, which is the whole point (and in some sense the main result) of the paper.

We just mentioned there is a principled way of speaking about dynamics under potentials with some geometric meaning. Indeed, when the potential can be read as a deformation of the space, geodesic curves on that deformed geometry can be thought of as parallel transportation. Suppose we have an ensemble of states and want to lift those states into some other space encoding some extra information, like a space of scalars attaching a probability to each state. What that lift looks like now depends on the potential. When we do inference and try to learn probabilities over data, we are trying to find what that lift is—i.e., what is $$p(x)$$. Equating this with a simple flow in a potential has a nice interpretation as the classification of states by probability _not_ being the solution to a Fokker-Planck equation, but the parallel transport of points $$p(x)$$ as a quantity interacting with the potential on states. 

This view is implicitly taking $$(x, p(x))$$ as a _surface_ containing probabilistic points. Parallel transport means we describe the density as being made up of lines along some surface, whose shapes are determined by the constraints on the states underlying the surface. This in turn means a simple equation for a flow in a potential is sufficient to determine the shape of the density. No integrals necessary. So, incorporating the interactions of flows in the probability space over states with the potential into our lifting process gives a nice conceptual explanation of the effectiveness of maximum entropy, and should be possible using parallel transport. 

%quick note -- constraints also induce flows on X, since pullback one-form

The equation for parallel transport satisfies the Newtonian relation 

$$ \nabla \nabla p(x) = - \nabla (\nabla V(x) p(x)) $$

for the second-order spatial change in some point $$p(x)$$ and a potential function $$\grad V(x)$$. By the gradient theorem, when we integrate this (ignoring some boring technical details) we get the system of ODEs for parallel transport in a potential $$\grad V(x)$$,

$$ \nabla p(x) = - \nabla V(x) p(x) $$, 

whose solution is obviously $$\exp\{ -V(x) \}$$. 
