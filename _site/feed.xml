<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-11-10T13:27:39-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Dalton A R Sakthivadivel</title><subtitle>This is the personal website of Dalton Sakthivadivel.</subtitle><entry><title type="html">Making and Breaking Symmetries in Mind and Life</title><link href="http://localhost:4000/posts/2021/09/symmetry-issue/" rel="alternate" type="text/html" title="Making and Breaking Symmetries in Mind and Life" /><published>2021-09-30T00:00:00-04:00</published><updated>2021-09-30T00:00:00-04:00</updated><id>http://localhost:4000/posts/2021/09/symmetry-issue</id><content type="html" xml:base="http://localhost:4000/posts/2021/09/symmetry-issue/">&lt;p&gt;I’m now contributing to, and co-editing, a &lt;a href=&quot;https://www.mdpi.com/journal/symmetry/special_issues/Making_Breaking_Symmetries_Mind_Life&quot;&gt;special issue of the journal &lt;em&gt;Symmetry&lt;/em&gt;&lt;/a&gt;. The abstract I’ve promised (and on which I hope to deliver) is located at that link. This paper is something I’ve been thinking about for a while. It seems like it should be possible to connect the language of FEP with some of the objects that we understand well in mathematical physics, given the way FEP extends older variational and dynamical ideas in statistical physics. A re-axiomatisation of the free energy principle is exactly what I, a complex systems theorist after excessive mathematical rigour, find interesting, and may be useful to the larger community.&lt;/p&gt;

&lt;p&gt;Indeed, the unification of mathematical physics with biophysics and computer science aligns with my general philosophy quite strongly, as I think it’s the best way to understand such systems. It is for this reason that I really believe this is an interesting set of questions, framed masterfully by my co-editors. In particular, it was the lead editor, Adam Safron, who generated the list of questions and potential insights listed in the issue. It also seems that &lt;em&gt;Symmetry&lt;/em&gt; is the best venue in which to discuss such a wide-reaching issue.&lt;/p&gt;

&lt;p&gt;If you are someone who works at some intersection between computer science, biology, or physics, defined by symmetry or invariance, do contribute. My favourite phrase is ‘fee waiver,’ so as a policy internal to the editors (at least one editor, and indeed, they’re all wonderful people) every effort will be made to ensure it is worth it.&lt;/p&gt;</content><author><name></name></author><category term="projects and papers" /><summary type="html">I’m now contributing to, and co-editing, a special issue of the journal Symmetry. The abstract I’ve promised (and on which I hope to deliver) is located at that link. This paper is something I’ve been thinking about for a while. It seems like it should be possible to connect the language of FEP with some of the objects that we understand well in mathematical physics, given the way FEP extends older variational and dynamical ideas in statistical physics. A re-axiomatisation of the free energy principle is exactly what I, a complex systems theorist after excessive mathematical rigour, find interesting, and may be useful to the larger community.</summary></entry><entry><title type="html">Activation Functions and Models of Magnets</title><link href="http://localhost:4000/posts/2021/02/act-func/" rel="alternate" type="text/html" title="Activation Functions and Models of Magnets" /><published>2021-02-02T00:00:00-05:00</published><updated>2021-02-02T00:00:00-05:00</updated><id>http://localhost:4000/posts/2021/02/act-func</id><content type="html" xml:base="http://localhost:4000/posts/2021/02/act-func/">&lt;p&gt;As stated on the home page of this website, I’m going to start writing the occasional blog post about my work. At the time of writing, I had resisted the temptation for a while, but I think it would be nice to post about specific things (e.g. papers) every once in a while; since publishing in pure mathematics really is once in a while, it seems like it won’t be too much to keep on top of. This inaugural post concerns a paper that I preprinted some time ago, which discusses the activation function in neural networks. The work in question can be found here: &lt;a href=&quot;https://arxiv.org/abs/2102.04896&quot;&gt;Formalising the Use of the Activation Function in Neural Inference&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As someone who does occasional work on statistical mechanics I am quite familiar with the Ising model, and with &lt;em&gt;universality&lt;/em&gt;. If one is not, I have a pedagogical paper on the Ising model &lt;a href=&quot;https://darsakthi.github.io/texts-lec-notes/ising-tutorial&quot;&gt;here on this website&lt;/a&gt;, which I wrote for more-or-less this purpose—I thought it would be good to have a well-explained resource to refer to. Universality refers to the fact that certain classes of phase transition, which are nothing but changes in the characteristics of an object (such as changing from a solid to a liquid to a gas), share key dynamical features. In this sense, a model that exhibits a phase transition can be used to model certain other systems faithfully, even if they are quite different. Universality is explained well in &lt;a href=&quot;https://www.damtp.cam.ac.uk/user/tong/sft.html&quot;&gt;David Tong’s notes on statistical field theory&lt;/a&gt;, and in John Cardy’s &lt;em&gt;Scaling and Renormalisation in Statistical Physics&lt;/em&gt;. There are some interesting references in &lt;a href=&quot;https://twitter.com/stevenstrogatz/status/1392958445624696838&quot;&gt;this tweet&lt;/a&gt; too, and &lt;a href=&quot;https://twitter.com/nigelgoldenfeld/status/1394443499924230145&quot;&gt;this response&lt;/a&gt; to that tweet discusses the same thing that is covered in Chapter 3 of Tong’s notes.&lt;/p&gt;

&lt;p&gt;In the course of working on something else, I came across an interesting case of the Ising universality class as a kind of toy model of a neural network. This paper discusses exactly this, and I read it as motivating the activation function ML people are familiar with beyond ‘it works.’ This is something like my broader research mission, formulating complex system theories like ML or neuroscience axiomatically, so I thought it was worth writing up. In that sense, hopefully it goes beyond the curiosity of ‘a neural net looks a bit like an Ising model; isn’t that clever.’&lt;/p&gt;

&lt;p&gt;The way a simple neurone might be claimed to work is by the following essential anatomy of an action potential: the resting potential is slightly below ionic equilibrium, creating a concentration gradient of positive ions diffusing across the cell membrane. The way the neurone keeps the membrane potential low is by pumping these ions out as they diffuse in. When the voltage of the neural cell rises to a critical point, sensors in the neurone ‘un-gate’ the closed channels that prohibit mass diffusion and close the pumps getting rid of these ions, and positive ions flood the cell, going past even the equilibrium point. This is the characteristic spike in membrane potential that we read as an action potential, and foregoing some dynamical details about how exactly this model would work, it is sufficient to describe an action potential.  We’ll hold on to this thought for now, and lay out the necessary physics; then we’ll revisit this model.&lt;/p&gt;

&lt;p&gt;The Ising model, on the other hand, is a model of the electronic structure of a magnet. It is a lattice of atoms and their electrons, the latter of which possess a quantity called ‘spin’ from quantum mechanics. It isn’t important what spin is—and it probably doesn’t really have an intuitive explanation—other than to say that it points ‘up’ or ‘down.’ When all spins are aligned in one such direction, the metal has a magnetic moment from this alignment. The Ising model exhibits a phase transition wherein when the metal is heated, the energy added from the heat causes these electrons to start flipping their spins, just like molecules in a solid vibrating at high speeds when heated. This disorders the configuration of the lattice, and zeroes the magnetic moment \(m\). The contrapositive also holds: when the magnet is cooled once more, past a critical point \(m\) goes to one again. This is like a liquid solidifying below zero centigrade.&lt;/p&gt;

&lt;p&gt;One way of modelling the dynamics of the Ising model is by mean field theory (MFT), which looks at the ‘average’ dynamics of a system. The way we do MFT is by approximating the system with a simpler system that is related in some way to the original (specifically, we use a trial Hamiltonian for which the free energy is bounded from below by the Bogoliubov inequality). The typical way this is done, and in some sense the most meaningful, is by replacing the system with a ‘mean field’ and ignoring fluctuations in the dynamics of smaller items constituting the system. MFT is thus a theory of mean fields, ignoring the fluctuations around a mean (consider the transport relation \(\delta A(t) = A(t) - \langle A \rangle\) for the dynamics of an observable \(A\) in the presence of fluctuations. When we ignore fluctuations and the term \(\delta A(t)\) vanishes, we have \(A(t) = \langle A \rangle\)). It is also a multiscale modelling method, if these fluctuations are due to the dynamics of smaller things in a large system, and the mean field is representative of the dynamics of the larger things. In this sense MFT is not just a clever approximation—not in my view, anyway—but has some extra ontological importance as a method of understanding a system’s dynamics at multiple scales.&lt;/p&gt;

&lt;p&gt;In Section IIC, we do exactly this, derive a mean field model for the Ising model. It’s nothing new, I just go through it for the purpose of the paper. I actually use a short cut that neglects fluctuations, rather than performing the full formal derivation. This is, in fact, the same as the formal procedure—neglecting fluctuations produces a simpler trial Hamiltonian, for which the free energy is strictly greater than the real one, which we minimise with respect to a control parameter based on what we know of the Bogoliubov inequality. Again, it’s nothing but a short cut.&lt;/p&gt;

&lt;p&gt;Revisiting the model we discussed earlier, in Sections IIA and IIB we discuss the similarities between the Ising lattice and the network of channels in the cell membrane, with a spin equivalent to a channel containing a diffusing ion (\(+1\)) or not (\(-1\)). We consider the Ising model in a hot environment with an occasional quench applied, to model the effect of an external stimulus being applied that cools the lattice. In the neural case, this is a cell body in a disordered environment—some channels closed and some open—which occasionally is quenched and cooled below the critical temperature. When this cooling happens, all the channels open, the analogous Ising model is positively magnetised, and as the magnetic moment goes to one, we have an action potential. When the quench is removed, the neurone ‘heats up’ again, and the channels close just as spins get disordered.&lt;/p&gt;

&lt;p&gt;The key feature of this phase transition is, if we claim a neurone is in the Ising universality class, and the indicated magnetisation function (Figure 1 in the paper) arises from a mean field model of the Ising model, then the typical sigmoidal activation function is a mean field model of a real neurone. Correspondingly, if we think of real neurones as the thing to aspire to in building artificial neural networks—indeed, human brains are the best learners that we know of—then this explains why the sigmoid class of activation function has been so successful.&lt;/p&gt;

&lt;p&gt;If we then consider the quench in more detail, and look at it dynamically, then we can parameterise the function for \(m\) in terms of time and look at how ‘hard’ the quench is, and how long we take to heat up. If we assume a linear relationship between \(t\) and heat flow inwards, then the time spent magnetised is akin to a firing rate (i.e., spikes per second over the number of seconds gives us the number of spikes we have in the time we were allowed to spike). Analysing this gives us back the ReLU function and other functions with similar shapes.&lt;/p&gt;

&lt;p&gt;This paper is important to me for three reasons. For one thing, as stated previously, it formalises the problem of why sigmoidal activation functions are necessary to classify things, in a more motivated way than ‘it doesn’t work without it.’  For another, it shows that if we consider the conventional artificial neural network to be a mean field model of a real neural network, then this approximation of spiking dynamics necessarily looks as though it has a sigmoidal &lt;em&gt;or&lt;/em&gt; rectified activation function. For a third and final reason, it sheds light on other interesting questions about ANNs by way of statistical mechanical analogy, which are examined in the discussion.&lt;/p&gt;

&lt;p&gt;This has been a bit long, so hopefully it has some utility as a reading guide. I also hope one can see why I enjoyed writing this paper. What I will likely do from here onwards, if only to make this useful for myself, is deposit similar reading guides and summaries in this blog. Ideally, readers will also find it useful, and others end up enjoying my work too. Do let me know if this was the case for you—I’m always a fan of public outreach (read: not screaming into the void).&lt;/p&gt;</content><author><name></name></author><category term="projects and papers" /><category term="reading guides" /><summary type="html">As stated on the home page of this website, I’m going to start writing the occasional blog post about my work. At the time of writing, I had resisted the temptation for a while, but I think it would be nice to post about specific things (e.g. papers) every once in a while; since publishing in pure mathematics really is once in a while, it seems like it won’t be too much to keep on top of. This inaugural post concerns a paper that I preprinted some time ago, which discusses the activation function in neural networks. The work in question can be found here: Formalising the Use of the Activation Function in Neural Inference.</summary></entry></feed>