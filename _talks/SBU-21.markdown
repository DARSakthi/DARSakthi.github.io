---
title: "A higher gauge theory for machine learning and inference"
collection: talks
type: "Poster"
layout: talks
venue: "Stony Brook University"
date: 2021-05-05
location: "Stony Brook, New York, United States (virtually)"
---

I submitted this account of some preliminary work to a poster session for ECRs at my institution.

_Abstract_: 

We consider the mathematical theory underlying deep learning and machine learning, and attempt to motivate the success of such algorithms in finding solutions to 
dynamical problems. We use Jaynes’ Maximum Calibre, a form of Maximum Entropy, as a generalisation of machine learning-like inference. Borrowing from the principal bundle 
formalism underlying much of modern mathematical physics, we describe maximising calibre as a particular least action principle on the space of solutions of a dynamical system. 
We then prove an equivalence between the expressions for entropy maximisation under a constraint, and parallel transportation in the path space over a bundle. In doing so, 
we provide a theory justifying machine learning’s efficacy in solving arbitrarily complex data-generating processes.

Poster: [A Higher Gauge Theory for Machine Learning and Inference]({{site.files}}/files/Gauge Theory ML Poster.pdf)
